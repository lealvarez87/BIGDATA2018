{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CMCC](http://cmcc.ufabc.edu.br/images/logo_site.jpg)\n",
    "# **Spark + Python = PySpark**\n",
    "\n",
    "#### Esse notebook introduz os conceitos básicos do Spark através de sua interface com a linguagem Python. Como aplicação inicial faremos o clássico examplo de contador de palavras . Com esse exemplo é possível entender a lógica de programação funcional para as diversas tarefas de exploração de dados distribuídos.\n",
    "\n",
    "#### Para isso utilizaremos o livro texto [Trabalhos completos de William Shakespeare](http://www.gutenberg.org/ebooks/100) obtidos do  [Projeto Gutenberg](http://www.gutenberg.org/wiki/Main_Page).  Veremos que esse mesmo algoritmo pode ser empregado em textos de qualquer tamanho.\n",
    "\n",
    "#### ** Esse notebook contém:  **\n",
    "#### *Parte 1:* Criando uma base RDD e RDDs de tuplas\n",
    "#### *Parte 2:* Manipulando RDDs de tuplas\n",
    "#### *Parte 3:* Encontrando palavras únicas e calculando médias\n",
    "#### *Parte 4:* Aplicar contagem de palavras em um arquivo\n",
    "#### *Parte 5:* Similaridade entre Objetos\n",
    "#### Para os exercícios é aconselhável consultar a documentação da [API do PySpark](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Part 1: Criando e Manipulando RDDs **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nessa parte do notebook vamos criar uma base RDD a partir de uma lista com o comando `parallelize`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1a) Criando uma base RDD **\n",
    "#### Podemos criar uma base RDD de diversos tipos e fonte do Python com o comando `sc.parallelize(fonte, particoes)`, sendo fonte uma variável contendo os dados (ex.: uma lista) e particoes o número de partições para trabalhar em paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "ListaPalavras = ['gato', 'elefante', 'rato', 'rato', 'gato']\n",
    "palavrasRDD = sc.parallelize(ListaPalavras, 4)\n",
    "print(type(palavrasRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1b) Plural **\n",
    "\n",
    "#### Vamos criar uma função que transforma uma palavra no plural adicionando uma letra 's' ao final da string. Em seguida vamos utilizar a função `map()` para aplicar a transformação em cada palavra do RDD.\n",
    "\n",
    "#### Em Python (e muitas outras linguagens) a concatenação de strings é custosa. Uma alternativa melhor é criar uma nova string utilizando [`str.format()`](https://docs.python.org/2/library/string.html#format-string-syntax).\n",
    "\n",
    "#### Nota: a string entre os conjuntos de três aspas representa a documentação da função. Essa documentação é exibida com o comando `help()`. Vamos utilizar a padronização de documentação sugerida para o Python, manteremos essa documentação em inglês."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gatos\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def Plural(palavra):\n",
    "    \"\"\"Adds an 's' to `palavra`.\n",
    "\n",
    "    Args:\n",
    "        palavra (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: A string with 's' added to it.\n",
    "    \"\"\"\n",
    "    return str.format(palavra + \"s\")\n",
    "\n",
    "print(Plural('gato'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function Plural in module __main__:\n",
      "\n",
      "Plural(palavra)\n",
      "    Adds an 's' to `palavra`.\n",
      "    \n",
      "    Args:\n",
      "        palavra (str): A string.\n",
      "    \n",
      "    Returns:\n",
      "        str: A string with 's' added to it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Plural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert Plural('rato')=='ratos', 'resultado incorreto!'\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1c) Aplicando a função ao RDD **\n",
    "#### Transforme cada palavra do nosso RDD em plural usando [map()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.map) \n",
    "\n",
    "#### Em seguida, utilizaremos o comando  [collect()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.collect) que retorna a RDD como uma lista do Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralRDD = palavrasRDD.map(Plural)\n",
    "print (pluralRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** Nota: ** utilize o comando `collect()` apenas quando tiver certeza de que a lista caberá na memória. Para gravar os resultados de volta em arquivo texto ou base de dados utilizaremos outro comando."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1d) Utilizando uma função `lambda` **\n",
    "#### Repita a criação de um RDD de plurais, porém utilizando uma função lambda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gatos', 'elefantes', 'ratos', 'ratos', 'gatos']\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralLambdaRDD = palavrasRDD.map(lambda palavra : palavra + \"s\")\n",
    "print (pluralLambdaRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralLambdaRDD.collect()==['gatos','elefantes','ratos','ratos','gatos'], 'valores incorretos!'\n",
    "print ('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1e) Tamanho de cada palavra **\n",
    "#### Agora use `map()` e uma função `lambda` para retornar o número de caracteres em cada palavra. Utilize `collect()` para armazenar o resultado em forma de listas na variável destino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 9, 5, 5, 5]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "pluralTamanho = (pluralRDD\n",
    "                 .map(lambda palavra : len(palavra))\n",
    "                 .collect()\n",
    "                 )\n",
    "print (pluralTamanho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert pluralTamanho==[5,9,5,5,5], 'valores incorretos'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (1f) RDDs de pares e tuplas **\n",
    "\n",
    "#### Para contar a frequência de cada palavra de maneira distribuída, primeiro devemos atribuir um valor para cada palavra do RDD. Isso irá gerar um base de dados (chave, valor). Desse modo podemos agrupar a base através da chave, calculando a soma dos valores atribuídos. No nosso caso, vamos atribuir o valor `1` para cada palavra.\n",
    "\n",
    "#### Um RDD contendo a estrutura de tupla chave-valor `(k,v) ` é chamada de RDD de tuplas ou *pair RDD*.\n",
    "\n",
    "#### Vamos criar nosso RDD de pares usando a transformação  `map()` com uma função `lambda()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gato', 1), ('elefante', 1), ('rato', 1), ('rato', 1), ('gato', 1)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavraPar = palavrasRDD.map(lambda palavra : (palavra, 1))\n",
    "print (palavraPar.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "assert palavraPar.collect() == [('gato',1),('elefante',1),('rato',1),('rato',1),('gato',1)], 'valores incorretos!'\n",
    "print (\"Ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 2: Manipulando RDD de tuplas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vamos manipular nossa RDD para contar as palavras do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2a) Função `groupByKey()`  **\n",
    "\n",
    "#### A função [groupByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.groupByKey) agrupa todos os valores de um RDD através da chave (primeiro elemento da tupla) agregando os valores em uma lista.\n",
    "\n",
    "#### Essa abordagem tem um ponto fraco pois:\n",
    "  + #### A operação requer que os dados distribuídos sejam movidos em massa para que permaneçam na partição correta.\n",
    "  + #### As listas podem se tornar muito grandes. Imagine contar todas as palavras do Wikipedia: termos comuns como \"a\", \"e\" formarão uma lista enorme de valores que pode não caber na memória do processo escravo.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elefante: [1]\n",
      "rato: [1, 1]\n",
      "gato: [1, 1]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasGrupo = palavraPar.groupByKey()\n",
    "for chave, valor in palavrasGrupo.collect():\n",
    "    valores = list(valor)\n",
    "    print(f'{chave}: {valores}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(palavrasGrupo.mapValues(lambda x: list(x)).collect()) == [('elefante', [1]), ('gato',[1, 1]), ('rato',[1, 1])], 'Valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2b) Calculando as contagens **\n",
    "#### Após o  `groupByKey()` nossa RDD contém elementos compostos da palavra, como chave, e um iterador contendo todos os valores correspondentes aquela chave.\n",
    "#### Utilizando a transformação `map()` e a função `sum()`, contrua um novo RDD que consiste de tuplas (chave, soma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemGroup = palavrasGrupo.mapValues(lambda x : sum(x))\n",
    "print (contagemGroup.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagemGroup.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2c) `reduceByKey` **\n",
    "#### Um comando mais interessante para a contagem é o  [reduceByKey()](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.reduceByKey) que cria uma nova RDD de tuplas.\n",
    "\n",
    "#### Essa transformação aplica a transformação `reduce()` vista na aula anterior para os valores de cada chave. Dessa forma, a função de transformação pode ser aplicada em cada partição local para depois ser enviada para redistribuição de partições, reduzindo o total de dados sendo movidos e não mantendo listas grandes na memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagem = palavraPar.reduceByKey(lambda x,y:x+y)\n",
    "print( contagem.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagem.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (2d) Agrupando os comandos **\n",
    "\n",
    "#### A forma mais usual de realizar essa tarefa, partindo do nosso RDD palavrasRDD, é encadear os comandos map e reduceByKey em uma linha de comando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "contagemFinal = (palavrasRDD\n",
    "                 .map(lambda palavra : (palavra, 1))\n",
    "                 .reduceByKey(lambda x,y:x+y)                \n",
    "                 )\n",
    "print (contagemFinal.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contagemFinal.collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 3: Encontrando as palavras únicas e calculando a média de contagem **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3a) Palavras Únicas **\n",
    "\n",
    "#### Calcule a quantidade de palavras únicas do RDD. Utilize o mesmo pipeline anterior porém substituindo `.collect()` por outro método do API da RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "palavrasUnicas = (contagemFinal\n",
    "                  .count()\n",
    "                 )\n",
    "print (palavrasUnicas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert palavrasUnicas==3, 'valor incorreto!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (3b) Calculando a Média de contagem de palavras **\n",
    "\n",
    "#### Encontre a média de frequência das palavras utilizando o RDD `contagem`.\n",
    "\n",
    "#### Note que a função do comando `reduce()` é aplicada em cada tupla do RDD. Para realizar a soma das contagens, primeiro é necessário mapear o RDD para um RDD contendo apenas os valores das frequências (sem as chaves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1.67\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "# add é equivalente a lambda x,y: x+y\n",
    "from operator import add\n",
    "total = (contagemFinal\n",
    "         .map(lambda x: x[1])\n",
    "         .reduce(add)         \n",
    "         )\n",
    "media = total / float(palavrasUnicas)\n",
    "print (total)\n",
    "print (round(media, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert round(media, 2)==1.67, 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 4: Aplicar nosso algoritmo em um arquivo **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4a) Função `contaPalavras` **\n",
    "\n",
    "#### Para podermos aplicar nosso algoritmo genéricamente em diversos RDDs, vamos primeiro criar uma função para aplicá-lo em qualquer fonte de dados. Essa função recebe de entrada um RDD contendo uma lista de chaves (palavras) e retorna um RDD de tuplas com as chaves e a contagem delas nessa RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('elefante', 1), ('rato', 2), ('gato', 2)]\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "def contaPalavras(chavesRDD):\n",
    "    \"\"\"Creates a pair RDD with word counts from an RDD of words.\n",
    "\n",
    "    Args:\n",
    "        chavesRDD (RDD of str): An RDD consisting of words.\n",
    "\n",
    "    Returns:\n",
    "        RDD of (str, int): An RDD consisting of (word, count) tuples.\n",
    "    \"\"\"\n",
    "    return (chavesRDD\n",
    "            .map(lambda palavra : (palavra, 1))\n",
    "            .reduceByKey(add)\n",
    "           )\n",
    "\n",
    "print (contaPalavras(palavrasRDD).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert sorted(contaPalavras(palavrasRDD).collect())==[('elefante',1), ('gato',2), ('rato',2)], 'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4b) Normalizando o texto **\n",
    "\n",
    "#### Quando trabalhamos com dados reais, geralmente precisamos padronizar os atributos de tal forma que diferenças sutis por conta de erro de medição ou diferença de normatização, sejam desconsideradas. Para o próximo passo vamos padronizar o texto para:\n",
    "  + #### Padronizar a capitalização das palavras (tudo maiúsculo ou tudo minúsculo).\n",
    "  + #### Remover pontuação.\n",
    "  + #### Remover espaços no início e no final da palavra.\n",
    " \n",
    "#### Crie uma função `removerPontuacao` que converte todo o texto para minúscula, remove qualquer pontuação e espaços em branco no início ou final da palavra. Para isso, utilize a biblioteca [re](https://docs.python.org/2/library/re.html) para remover todo texto que não seja letra, número ou espaço, encadeando com as funções de string para remover espaços em branco e converter para minúscula (veja [Strings](https://docs.python.org/2/library/stdtypes.html?highlight=str.lower#string-methods))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ola quem esta ai\n",
      "sem espaco esublinhado\n"
     ]
    }
   ],
   "source": [
    "# EXERCICIO\n",
    "import re\n",
    "def removerPontuacao(texto):\n",
    "    \"\"\"Removes punctuation, changes to lower case, and strips leading and trailing spaces.\n",
    "\n",
    "    Note:\n",
    "        Only spaces, letters, and numbers should be retained.  Other characters should should be\n",
    "        eliminated (e.g. it's becomes its).  Leading and trailing spaces should be removed after\n",
    "        punctuation is removed.\n",
    "\n",
    "    Args:\n",
    "        texto (str): A string.\n",
    "\n",
    "    Returns:\n",
    "        str: The cleaned up string.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^A-Za-z0-9 ]', '', texto).strip().lower()\n",
    "print (removerPontuacao('Ola, quem esta ai??!'))\n",
    "print (removerPontuacao(' Sem espaco e_sublinhado!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "assert removerPontuacao(' O uso de virgulas, embora permitido, nao deve contar. ')=='o uso de virgulas embora permitido nao deve contar', 'string incorreta!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4c) Carregando arquivo texto  **\n",
    "\n",
    "#### Para a próxima parte vamos utilizar o livro [Trabalhos completos de William Shakespeare](http://www.gutenberg.org/ebooks/100) do [Projeto Gutenberg](http://www.gutenberg.org/wiki/Main_Page). \n",
    "\n",
    "#### Para converter um texto em uma RDD, utilizamos a função `textFile()` que recebe como entrada o nome do arquivo texto que queremos utilizar e o número de partições.\n",
    "\n",
    "#### O nome do arquivo texto pode se referir a um arquivo local ou uma URI de arquivo distribuído (ex.: hdfs://).\n",
    "\n",
    "#### Vamos também aplicar a função `removerPontuacao()` para normalizar o texto e verificar as 15 primeiras linhas com o comando `take()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o199.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/leandro/Documentos/Mestrado/Inteligência na Web e Big Data/BIGDATA2018/Atividade02/Data/pg100.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-2a8d256804da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Ex.: sc.parallelize(['gato','cachorro','boi']).zipWithIndex() ==> [('gato',0), ('cachorro',1), ('boi',2)]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# sep.join() junta as strings de uma lista através do separador sep. Ex.: ','.join(['a','b','c']) ==> 'a,b,c'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m print ('\\n'.join(shakesRDD\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mzipWithIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mlinha\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'{0}: {1}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mzipWithIndex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2167\u001b[0m         \"\"\"\n\u001b[1;32m   2168\u001b[0m         \u001b[0mstarts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2169\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2170\u001b[0m             \u001b[0mnums\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnums\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1160\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    318\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    319\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    321\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o199.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/leandro/Documentos/Mestrado/Inteligência na Web e Big Data/BIGDATA2018/Atividade02/Data/pg100.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "# Apenas execute a célula\n",
    "import os.path\n",
    "\n",
    "arquivo = os.path.join('Data', 'pg100.txt') \n",
    "\n",
    "# lê o arquivo com textFile e aplica a função removerPontuacao        \n",
    "shakesRDD = (sc\n",
    "             .textFile(arquivo, 8)\n",
    "             .map(removerPontuacao)\n",
    "             )\n",
    "\n",
    "# zipWithIndex gera tuplas (conteudo, indice) onde indice é a posição do conteudo na lista sequencial\n",
    "# Ex.: sc.parallelize(['gato','cachorro','boi']).zipWithIndex() ==> [('gato',0), ('cachorro',1), ('boi',2)]\n",
    "# sep.join() junta as strings de uma lista através do separador sep. Ex.: ','.join(['a','b','c']) ==> 'a,b,c'\n",
    "print ('\\n'.join(shakesRDD\n",
    "                .zipWithIndex()\n",
    "                .map(lambda linha: '{0}: {1}'.format(linha[0],linha[1]))\n",
    "                .take(15)\n",
    "               ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4d) Extraindo as palavras **\n",
    "#### Antes de poder usar nossa função  `contaPalavras()`, temos ainda que trabalhar em cima da nossa RDD:\n",
    "  + #### Precisamos gerar listas de palavras ao invés de listas de sentenças.\n",
    "  + #### Eliminar linhas vazias.\n",
    " \n",
    "#### As strings em Python tem o método [split()](https://docs.python.org/2/library/string.html#string.split) que faz a separação de uma string por separador. No nosso caso, queremos separar as strings por espaço. \n",
    "\n",
    "#### Utilize a função `map()` para gerar um novo RDD como uma lista de palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.map(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print (shakesPalavrasRDD.take(5))\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conforme deve ter percebido, o uso da função `map()` gera uma lista para cada linha, criando um RDD contendo uma lista de listas.\n",
    "\n",
    "#### Para resolver esse problema, o Spark possui uma função análoga chamada `flatMap()` que aplica a transformação do `map()`, porém *achatando* o retorno em forma de lista para uma lista unidimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "shakesPalavrasRDD = shakesRDD.flatMap(lambda x: x.split())\n",
    "total = shakesPalavrasRDD.count()\n",
    "print (shakesPalavrasRDD.take(5))\n",
    "print (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert total==959359, \"valor incorreto de palavras!\"\n",
    "print (\"OK\")\n",
    "assert shakesPalavrasRDD.take(5)==['project', 'gutenbergs', 'the', 'complete', 'works'],'lista incorreta de palavras'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparem que o `flatMap` já eliminou as entradas vazias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (4f) Contagem de palavras **\n",
    "#### Agora que nossa RDD contém uma lista de palavras, podemos aplicar nossa função `contaPalavras()`.\n",
    "\n",
    "#### Aplique a função em nossa RDD e utilize a função `takeOrdered` para imprimir as 15 palavras mais frequentes.\n",
    "\n",
    "#### `takeOrdered()` pode receber um segundo parâmetro que instrui o Spark em como ordenar os elementos. Ex.:\n",
    "\n",
    "#### `takeOrdered(15, key=lambda x: -x)`: ordem decrescente dos valores de x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "top15 = contaPalavras(shakesPalavrasRDD).takeOrdered(15, key=lambda x: -x[1])\n",
    "print ('\\n'.join(map(lambda x: f'{x[0]}: {x[1]}', top15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert top15 == [('the', 29996), ('and', 28353), ('i', 21860), ('to', 20885), ('of', 18811), ('a', 15992), ('you', 14439), ('my', 13191), ('in', 12027), ('that', 11782), ('is', 9711), ('not', 9068), ('with', 8521), ('me', 8271), ('for', 8184)],'valores incorretos!'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** Parte 5: Similaridade entre Objetos **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nessa parte do laboratório vamos aprender a calcular a distância entre atributos numéricos, categóricos e textuais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5a) Vetores no espaço Euclidiano **\n",
    "\n",
    "#### Quando nossos objetos são representados no espaço Euclidiano, medimos a similaridade entre eles através da *p-Norma* definida por:\n",
    "\n",
    "#### $$d(x,y,p) = (\\sum_{i=1}^{n}{|x_i - y_i|^p})^{1/p}$$\n",
    "\n",
    "#### As normas mais utilizadas são $p=1,2,\\infty$ que se reduzem em distância absoluta, Euclidiana e máxima distância:\n",
    "\n",
    "#### $$d(x,y,1) = \\sum_{i=1}^{n}{|x_i - y_i|}$$\n",
    "\n",
    "#### $$d(x,y,2) = (\\sum_{i=1}^{n}{|x_i - y_i|^2})^{1/2}$$\n",
    "\n",
    "#### $$d(x,y,\\infty) = \\max(|x_1 - y_1|,|x_2 - y_2|, ..., |x_n - y_n|)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Vamos criar uma função pNorm que recebe como parâmetro p e retorna uma função que calcula a pNorma\n",
    "def pNorm(p):\n",
    "    \"\"\"Generates a function to calculate the p-Norm between two points.\n",
    "\n",
    "    Args:\n",
    "        p (int): The integer p.\n",
    "\n",
    "    Returns:\n",
    "        Dist: A function that calculates the p-Norm.\n",
    "    \"\"\"\n",
    "\n",
    "    def Dist(x,y):\n",
    "        return np.power(np.power(np.abs(x-y),p).sum(),1/float(p))\n",
    "    return Dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores numéricos\n",
    "np.random.seed(42)\n",
    "numPointsRDD = sc.parallelize(enumerate(np.random.random(size=(10,100))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartPointsRDD = numPointsRDD.cartesian(numPointsRDD)\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "#print(cartPointsRDD.take(1))\n",
    "cartPointsParesRDD = cartPointsRDD.map(lambda x: ((x[0][0], x[1][0]), (x[0][1], x[1][1])))\n",
    "\n",
    "# Aplique um mapa para calcular a Distância Euclidiana entre os pares\n",
    "Euclid = pNorm(2)\n",
    "distRDD = cartPointsParesRDD.map(lambda x: (x[0], Euclid(x[1][0], x[1][1])))\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statRDD = distRDD.map(lambda x: x[1])\n",
    "\n",
    "minv, maxv, meanv = statRDD.min(), statRDD.max(), statRDD.mean()\n",
    "print (minv, maxv, meanv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (minv.round(2), maxv.round(2), meanv.round(2))==(0.0, 4.71, 3.75), 'Valores incorretos'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ** (5b) Valores Categóricos **\n",
    "\n",
    "#### Quando nossos objetos são representados por atributos categóricos, eles não possuem uma similaridade espacial. Para calcularmos a similaridade entre eles podemos primeiro transformar nosso vetor de atrbutos em um vetor binário indicando, para cada possível valor de cada atributo, se ele possui esse atributo ou não.\n",
    "\n",
    "#### Com o vetor binário podemos utilizar a distância de Hamming  definida por:\n",
    "\n",
    "#### $$ H(x,y) = \\sum_{i=1}^{n}{x_i != y_i} $$\n",
    "\n",
    "#### Também é possível definir a distância de Jaccard como:\n",
    "\n",
    "#### $$ J(x,y) = \\frac{\\sum_{i=1}^{n}{x_i == y_i} }{\\sum_{i=1}^{n}{\\max(x_i, y_i}) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma função para calcular a distância de Hamming\n",
    "def Hamming(x,y):\n",
    "    \"\"\"Calculates the Hamming distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        H (int): The Hamming distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x!=y).sum()\n",
    "\n",
    "# Vamos criar uma função para calcular a distância de Jaccard\n",
    "def Jaccard(x,y):\n",
    "    \"\"\"Calculates the Jaccard distance between two binary vectors.\n",
    "\n",
    "    Args:\n",
    "        x, y (np.array): Array of binary integers x and y.\n",
    "\n",
    "    Returns:\n",
    "        J (int): The Jaccard distance between x and y.\n",
    "    \"\"\"\n",
    "    return (x==y).sum()/float( np.maximum(x,y).sum() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos criar uma RDD com valores categóricos\n",
    "catPointsRDD = sc.parallelize(enumerate([['alto', 'caro', 'azul'],\n",
    "                             ['medio', 'caro', 'verde'],\n",
    "                             ['alto', 'barato', 'azul'],\n",
    "                             ['medio', 'caro', 'vermelho'],\n",
    "                             ['baixo', 'barato', 'verde'],\n",
    "                            ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Crie um RDD de chaves únicas utilizando flatMap\n",
    "chavesRDD = (catPointsRDD\n",
    "             .flatMap(lambda palavra : palavra[1])\n",
    "             .map(lambda palavra : (palavra,1))\n",
    "             .reduceByKey(add) \n",
    "             .map(lambda palavra: palavra[0])\n",
    "            )\n",
    "\n",
    "print(chavesRDD.collect())             \n",
    "\n",
    "chaves = dict((v,k) for k,v in enumerate(chavesRDD.collect()))\n",
    "nchaves = len(chaves)\n",
    "print(chaves, nchaves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert chaves=={'baixo': 0, 'barato': 1, 'alto': 2, 'medio': 3, 'verde': 4, 'vermelho': 5, 'caro': 6, 'azul': 7}, 'valores incorretos!'\n",
    "print (\"OK\")\n",
    "\n",
    "assert nchaves==8, 'número de chaves incorreta'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateNP(atributos,chaves):  \n",
    "    \"\"\"Binarize the categorical vector using a dictionary of keys.\n",
    "\n",
    "    Args:\n",
    "        atributos (list): List of attributes of a given object.\n",
    "        chaves (dict): dictionary with the relation attribute -> index\n",
    "\n",
    "    Returns:\n",
    "        array (np.array): Binary array of attributes.\n",
    "    \"\"\"\n",
    "    \n",
    "    array = np.zeros(len(chaves))\n",
    "    for atr in atributos:\n",
    "        array[ chaves[atr] ] = 1\n",
    "    return array\n",
    "\n",
    "# Converte o RDD para o formato binário, utilizando o dict chaves\n",
    "binRDD = catPointsRDD.map(lambda rec: (rec[0],CreateNP(rec[1], chaves)))\n",
    "binRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICIO\n",
    "# Procure dentre os comandos do PySpark, um que consiga fazer o produto cartesiano da base com ela mesma\n",
    "cartBinRDD = binRDD.cartesian(binRDD)\n",
    "\n",
    "# Aplique um mapa para transformar nossa RDD em uma RDD de tuplas ((id1,id2), (vetor1,vetor2))\n",
    "# DICA: primeiro utilize o comando take(1) e imprima o resultado para verificar o formato atual da RDD\n",
    "#print(cartBinRDD.take(1))\n",
    "cartBinParesRDD = cartBinRDD.map(lambda x: ((x[0][0], x[1][0]), (x[0][1], x[1][1])))\n",
    "\n",
    "# Aplique um mapa para calcular a Distância de Hamming e Jaccard entre os pares\n",
    "hamRDD = cartBinParesRDD.map(lambda x: Hamming(x[1][0], x[1][1]))\n",
    "jacRDD = cartBinParesRDD.map(lambda x: Jaccard(x[1][0], x[1][1]))\n",
    "\n",
    "# Encontre a distância máxima, mínima e média, aplicando um mapa que transforma (chave,valor) --> valor\n",
    "# e utilizando os comandos internos do pyspark para o cálculo da min, max, mean\n",
    "statHRDD = hamRDD.map(lambda x : x)\n",
    "statJRDD = jacRDD.map(lambda x : x)\n",
    "\n",
    "Hmin, Hmax, Hmean = statHRDD.min(), statHRDD.max(), statHRDD.mean()\n",
    "Jmin, Jmax, Jmean = statJRDD.min(), statJRDD.max(), statJRDD.mean()\n",
    "\n",
    "print (\"\\t\\tMin\\tMax\\tMean\")\n",
    "print (\"Hamming:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(Hmin, Hmax, Hmean ))\n",
    "print (\"Jaccard:\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format( Jmin, Jmax, Jmean ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (Hmin.round(2), Hmax.round(2), Hmean.round(2)) == (0.00,6.00,3.52), 'valores incorretos'\n",
    "print (\"OK\")\n",
    "assert (Jmin.round(2), Jmax.round(2), Jmean.round(2)) == (0.33,2.67,1.14), 'valores incorretos'\n",
    "print (\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
